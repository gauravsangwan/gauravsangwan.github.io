---
title: "Secure Deep Learning Pipeline Design for Vision transformer"
collection: publications
permalink: /publication/rectify-this
excerpt: 'This work introduces a lightweight adversarial detection filter that flags malicious inputs before they reach the model, offering an efficient and adaptable solution for enhancing the security of deep learning systems.'
date: 2024-6-20
venue: 'Peer Reviewed Core-A-ranking Computer Architecture Journal'
paperurl: 'https://github.com/gauravsangwan/HAVIT/tree/main'
---

<!-- - **DOI**: [https://doi.org/10.1609/aaai.v37i13.26942](https://doi.org/10.1609/aaai.v37i13.26942) -->
- **Keywords**: Vision Transformer, Hardware Acceleration, Data-pipeline Manipulation, Adversarial Defenses, Lightweight
- This paper addresses the vulnerability of Vision Transformers (ViTs) and other deep neural networks (DNNs) to adversarial attacks, which involve subtle perturbations that can lead to incorrect model predictions. These attacks pose significant security risks in real-world applications like autonomous driving and image classification. While many defenses have been developed, few focus on detecting adversarial inputs before they reach the model. The authors propose a lightweight adversarial detection method that acts as a filter, flagging potentially harmful inputs prior to processing. This method is computationally efficient, adaptable to various models, and enhances the performance of existing defense mechanisms.
