---
title: "Efficient Hardware Design for Vision transformer"
collection: publications
permalink: /publication/havit
excerpt: 'Transformers have served as a strong foundation in NLP and CV domain, but for practical application a dedicated hardware accelerator is required to increase throughput while keeping energy consumption lower than GPU.'
date: 2024-6-20
venue: 'Peer Reviewed Core-A-ranking Computer Architecture Journal'
paperurl: 'https://github.com/gauravsangwan/HAVIT/tree/main'
---

<!-- - **DOI**: [https://doi.org/10.1609/aaai.v37i13.26942](https://doi.org/10.1609/aaai.v37i13.26942) -->
- **Keywords**: Vision Transformer, Hardware Acceleration, Data-pipeline Manipulation
- The paper presents an optimized vision transformer (ViT) accelerator aimed at improving computational efficiency on devices with limited resources. By employing lightweight techniques for selecting key image patches, It minimizes the need for intensive computation typically required by ViTs. These techniques use simple, low-cost methods to identify relevant data, enabling more efficient processing. The accelerator further enhances performance through parallelism while maintaining energy efficiency, offering a practical solution for real-time computer vision tasks.
